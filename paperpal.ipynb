{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11462801,"sourceType":"datasetVersion","datasetId":7182893},{"sourceId":11462824,"sourceType":"datasetVersion","datasetId":7182908},{"sourceId":11462993,"sourceType":"datasetVersion","datasetId":7183025}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:32:50.721377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain langchain-community pypdf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install PyPDF2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import PyPDF2\n\ndef load_pdf_text(pdf_path):\n    text = \"\"\n    with open(pdf_path, \"rb\") as f:\n        reader = PyPDF2.PdfReader(f)\n        for page in reader.pages:\n            text += page.extract_text()\n    return text\n\n# Example use\npdf_path = \"/kaggle/input/testing/testing.pdf\"  # update this\npdf_text = load_pdf_text(pdf_path)\n\nprint(pdf_text[:1000])  # print first 1000 characters\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import openai\nimport os\n\n# Set the Gemini API Key\ngemini_api_key = \"gemini-api-key\"\n\n# Set up your API client (if needed based on Gemini API docs)\nopenai.api_key = gemini_api_key\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\n\ndef split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len\n    )\n    chunks = text_splitter.split_text(text)\n    documents = [Document(page_content=chunk) for chunk in chunks]\n    return documents\n\n# Split the text\ndocuments = split_text_into_chunks(pdf_text)\n\nprint(f\"Total chunks created: {len(documents)}\")\nprint(documents[0].page_content[:500])  # Show preview of first chunk\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q google-generativeai\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\n\n# Replace with your actual API key\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\n\n# Paste your API key here\nos.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\"\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Wrap the Gemini API call into a reusable function","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")  # keep it private!\nmodel = genai.GenerativeModel(\"gemini-1.5-pro\")\n\nresponse = model.generate_content(\"Summarize the causes of World War I in 5 points.\")\nprint(response.text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U google-generativeai\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")\n\n# List available models to find the correct one\nmodels = genai.list_models()\nfor m in models:\n    print(m.name, \"‚Üí\", m.supported_generation_methods)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")  # replace with your actual API key\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n\nprompt = \"\"\"\n1. **Entangling Alliances:** A complex web of military alliances obligated nations to defend one another, turning a regional conflict into a continental war.\n2. **Militarism:** The aggressive build-up of armed forces heightened tensions and prepared nations for large-scale war.\n3. **Imperialism:** Competing colonial ambitions intensified global rivalries, especially among European powers.\n4. **Nationalism:** Intense national pride and ethnic tensions fueled conflicts, particularly in the Balkans.\n\n**Conclusion:** These intertwined causes‚Äîalliances, militarism, imperialism, and nationalism‚Äîcreated a volatile environment that exploded into World War I following the assassination of Archduke Franz Ferdinand.\n\"\"\"\n\nresponse = model.generate_content(prompt)\nprint(response.text)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install streamlit\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\nimport google.generativeai as genai\n\n# Configure the API key\ngenai.configure(api_key='AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas')\n\n# Load Gemini model\nmodel = genai.GenerativeModel('models/gemini-1.5-pro')\n\n# Streamlit UI\nst.set_page_config(page_title=\"GenAI Assistant\", layout=\"centered\")\nst.title(\"üéì GenAI-Powered Student Assistant\")\n\nprompt = st.text_area(\"Enter your academic question or prompt:\")\n\nif st.button(\"Generate Response\"):\n    if prompt.strip() == \"\":\n        st.warning(\"Please enter a prompt.\")\n    else:\n        with st.spinner(\"Thinking...\"):\n            try:\n                response = model.generate_content(prompt)\n                st.success(\"Response generated!\")\n                st.markdown(response.text)\n            except Exception as e:\n                st.error(f\"Error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q streamlit pyngrok google-generativeai\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nimport google.generativeai as genai\n\n# Configure your Gemini API key here\ngenai.configure(api_key='AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas')  # Replace with your key\n\n# Load the Gemini model\nmodel = genai.GenerativeModel('models/gemini-1.5-pro')\n\nst.set_page_config(page_title=\"GenAI Assistant\", layout=\"centered\")\nst.title(\"üéì GenAI-Powered Student Assistant\")\n\nprompt = st.text_area(\"Enter your academic question or prompt:\")\n\nif st.button(\"Generate Response\"):\n    if prompt.strip() == \"\":\n        st.warning(\"Please enter a prompt.\")\n    else:\n        with st.spinner(\"Thinking...\"):\n            try:\n                response = model.generate_content(prompt)\n                st.success(\"Response generated!\")\n                st.markdown(response.text)\n            except Exception as e:\n                st.error(f\"Error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok authtoken 2vvz3GRDSPY5qqYnaKM8cUzRUan_3KggFHXVDNo9q9QAbHdZW","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\nimport threading\nimport os\n\n# Run Streamlit in background\ndef run():\n    os.system('streamlit run app.py')\n\nthread = threading.Thread(target=run)\nthread.start()\n\n# Wait for it to start and then expose\npublic_url = ngrok.connect(8501)\nprint(f\"üåê Your app is live at: {public_url}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install google-generativeai\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nimport streamlit as st\n\n# Load your Gemini API key\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Streamlit App\nst.set_page_config(page_title=\"Student Assistant\", page_icon=\"üéì\")\nst.title(\"üéì GenAI-Powered Student Assistant\")\nst.markdown(\"Ask any academic-related question!\")\n\n# Create a Gemini model\nmodel = genai.GenerativeModel(\"gemini-pro\")\n\n# Chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# Show previous messages\nfor msg in st.session_state.messages:\n    with st.chat_message(msg[\"role\"]):\n        st.markdown(msg[\"content\"])\n\n# Take user input\nuser_prompt = st.chat_input(\"Type your academic question...\")\n\nif user_prompt:\n    # Add user input to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})\n\n    # Display user message\n    with st.chat_message(\"user\"):\n        st.markdown(user_prompt)\n\n    # Get response from Gemini\n    with st.chat_message(\"assistant\"):\n        response = model.generate_content(user_prompt)\n        st.markdown(response.text)\n\n    # Save assistant response\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.text})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\nimport google.generativeai as genai\nimport os\nfrom pyngrok import ngrok\n\n# Setup Gemini\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")\nmodel = genai.GenerativeModel('gemini-pro')\n\n# Sidebar\nst.sidebar.title(\"üìö Chatbot Categories\")\ncategory = st.sidebar.selectbox(\"Select a help category\", [\n    \"General Query\",\n    \"Subject Help\",\n    \"Study Planner\",\n    \"Concept Doubt\",\n    \"Note/Summary Generator\"\n])\n\n# Prefill template based on category\nif category == \"Subject Help\":\n    default_prompt = \"Explain the concept of Inheritance in Java.\"\nelif category == \"Study Planner\":\n    default_prompt = \"Create a 1-week study plan for Cloud Computing.\"\nelif category == \"Concept Doubt\":\n    default_prompt = \"I'm confused about normalization in DBMS. Can you explain?\"\nelif category == \"Note/Summary Generator\":\n    default_prompt = \"Summarize the basics of Machine Learning.\"\nelse:\n    default_prompt = \"\"\n\n# Main UI\nst.title(\"üéì GenAI Student Assistant\")\nuser_input = st.text_area(\"Ask me anything academic üëá\", value=default_prompt, height=100)\n\nif st.button(\"Ask\"):\n    if user_input:\n        with st.spinner(\"Thinking...\"):\n            response = model.generate_content(user_input)\n            st.markdown(\"### üì¢ Response:\")\n            st.write(response.text)\n    else:\n        st.warning(\"Please enter a question.\")\n\n# Expose app using ngrok (optional for Kaggle notebooks)\n# public_url = ngrok.connect(8501)\n# st.write(f\"üåê Your app is live at: {public_url}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install PyMuPDF\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF\n\nst.markdown(\"---\")\nst.subheader(\"üìÅ Upload a File to Ask from\")\n\nuploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n\nif uploaded_file:\n    # Read text from uploaded PDF\n    with fitz.open(stream=uploaded_file.read(), filetype=\"pdf\") as doc:\n        file_text = \"\"\n        for page in doc:\n            file_text += page.get_text()\n\n    st.success(\"‚úÖ File uploaded and processed.\")\n\n    user_question = st.text_area(\"Ask a question based on the file\", height=100)\n    if st.button(\"Ask from File\"):\n        if user_question:\n            with st.spinner(\"Thinking with your file...\"):\n                prompt = f\"\"\"Based on the following uploaded material, answer this question:\n\nMaterial:\n{file_text}\n\nQuestion:\n{user_question}\n\"\"\"\n                response = model.generate_content(prompt)\n                st.markdown(\"### üì¢ Response:\")\n                st.write(response.text)\n        else:\n            st.warning(\"Please type a question based on the file.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install python-docx\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # for PDFs\nimport docx  # for DOCX\nimport io\n\nst.markdown(\"---\")\nst.subheader(\"üìÅ Upload Study Material\")\n\nuploaded_file = st.file_uploader(\"Upload a file (PDF, DOCX, or TXT)\", type=[\"pdf\", \"docx\", \"txt\"])\n\nfile_text = \"\"\n\nif uploaded_file:\n    file_type = uploaded_file.name.split('.')[-1]\n\n    if file_type == \"pdf\":\n        with fitz.open(stream=uploaded_file.read(), filetype=\"pdf\") as doc:\n            for page in doc:\n                file_text += page.get_text()\n\n    elif file_type == \"docx\":\n        doc = docx.Document(uploaded_file)\n        for para in doc.paragraphs:\n            file_text += para.text + \"\\n\"\n\n    elif file_type == \"txt\":\n        stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n        file_text = stringio.read()\n\n    st.success(\"‚úÖ File processed successfully!\")\n\n    user_question = st.text_area(\"Ask something based on your file:\", height=100)\n    if st.button(\"Ask from File\"):\n        if user_question:\n            with st.spinner(\"Thinking...\"):\n                prompt = f\"\"\"Answer this based on the uploaded content:\n\nMaterial:\n{file_text}\n\nQuestion:\n{user_question}\n\"\"\"\n                response = model.generate_content(prompt)\n                st.markdown(\"### üì¢ Response:\")\n                st.write(response.text)\n        else:\n            st.warning(\"Please type a question.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fitz  # PyMuPDF for PDFs\nimport docx  # for DOCX\nimport io\n\n# ‚¨áÔ∏è This is the chunking function\ndef chunk_text(text, chunk_size=3000):\n    chunks = []\n    while len(text) > chunk_size:\n        split_point = text.rfind('.', 0, chunk_size)\n        if split_point == -1:\n            split_point = chunk_size\n        chunks.append(text[:split_point])\n        text = text[split_point:]\n    chunks.append(text)\n    return chunks\n\nst.subheader(\"üìÅ Upload Study Material\")\n\nuploaded_file = st.file_uploader(\"Upload a file (PDF, DOCX, or TXT)\", type=[\"pdf\", \"docx\", \"txt\"])\nfile_text = \"\"\n\nif uploaded_file:\n    file_type = uploaded_file.name.split('.')[-1]\n\n    if file_type == \"pdf\":\n        with fitz.open(stream=uploaded_file.read(), filetype=\"pdf\") as doc:\n            for page in doc:\n                file_text += page.get_text()\n\n    elif file_type == \"docx\":\n        doc = docx.Document(uploaded_file)\n        for para in doc.paragraphs:\n            file_text += para.text + \"\\n\"\n\n    elif file_type == \"txt\":\n        stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n        file_text = stringio.read()\n\n    st.success(\"‚úÖ File processed successfully!\")\n\n    # User asks a question\n    user_question = st.text_area(\"Ask something based on your file:\", height=100)\n    if st.button(\"Ask from File\"):\n        if user_question:\n            with st.spinner(\"Thinking...\"):\n                chunks = chunk_text(file_text)\n                combined_answer = \"\"\n\n                for i, chunk in enumerate(chunks):\n                    sub_prompt = f\"\"\"Based on this content, answer the question:\n\nMaterial Chunk {i+1}:\n{chunk}\n\nQuestion:\n{user_question}\n\"\"\"\n                    response = model.generate_content(sub_prompt)\n                    combined_answer += response.text + \"\\n\"\n\n                st.markdown(\"### üì¢ Final Response:\")\n                st.write(combined_answer)\n        else:\n            st.warning(\"Please type a question.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Disconnect all active tunnels\nngrok.kill()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Start a new tunnel\npublic_url = ngrok.connect(8501)\nprint(f\"üåê Your app is live at: {public_url}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\n\n# File uploader\nuploaded_file = st.file_uploader(\"Choose a file\", type=['csv', 'xlsx', 'txt', 'jpg', 'png'])\n\n# If a file is uploaded\nif uploaded_file is not None:\n    # Display the file name\n    st.write(f\"File {uploaded_file.name} uploaded successfully!\")\n\n    # For example, if the file is a CSV, display it as a dataframe\n    if uploaded_file.name.endswith('.csv'):\n        import pandas as pd\n        df = pd.read_csv(uploaded_file)\n        st.write(df)\n\n    # You can also show images if the file is an image\n    if uploaded_file.name.endswith(('jpg', 'png')):\n        from PIL import Image\n        img = Image.open(uploaded_file)\n        st.image(img)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\n\n# Title of the app\nst.title('File Upload App')\n\n# Create file uploader\nuploaded_file = st.file_uploader(\"Choose a file\")\n\n# Check if a file is uploaded\nif uploaded_file is not None:\n    st.write(\"File uploaded successfully!\")\n    st.write(uploaded_file.name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the app.py file\ncode = \"\"\"\nimport streamlit as st\n\n# Title of the app\nst.title('File Upload App')\n\n# Create file uploader\nuploaded_file = st.file_uploader(\"Choose a file\")\n\n# Check if a file is uploaded\nif uploaded_file is not None:\n    st.write(\"File uploaded successfully!\")\n    st.write(uploaded_file.name)\n\"\"\"\n\n# Save it to app.py\nwith open(\"app.py\", \"w\") as f:\n    f.write(code)\n\nprint(\"app.py file created successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# List files in the current directory\nos.listdir()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\nimport os\nimport subprocess\nimport time\n\n# Set up a tunnel for Streamlit app\npublic_url = ngrok.connect(8501)  # Correct the port by using a number, not a string\n\n# Command to run Streamlit app\ncommand = ['streamlit', 'run', 'app.py']\n\n# Start Streamlit app in the background\np = subprocess.Popen(command)\n\n# Wait for the app to start\ntime.sleep(10)\n\n# Show the public URL\nprint(f\"Streamlit app is running at: {public_url}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Disconnect all active tunnels\nngrok.kill()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\nfrom PyPDF2 import PdfReader\n\n# Function to extract text from the uploaded PDF file\ndef extract_text_from_pdf(pdf_file):\n    pdf_reader = PdfReader(pdf_file)\n    text = \"\"\n    for page_num in range(len(pdf_reader.pages)):\n        page = pdf_reader.pages[page_num]\n        text += page.extract_text()\n    return text\n\n# Set up Streamlit app\nst.title(\"File Upload and Query App\")\n\n# Upload file section\nuploaded_file = st.file_uploader(\"Choose a file\", type=[\"pdf\"])\n\nif uploaded_file is not None:\n    # Extract text from the uploaded PDF\n    st.write(\"File uploaded successfully!\")\n    text = extract_text_from_pdf(uploaded_file)\n    \n    # Display a portion of the extracted text (for user reference)\n    st.subheader(\"Extracted Text:\")\n    st.text_area(\"Extracted Text\", text[:1000], height=300)  # Display the first 1000 characters for preview\n\n    # User query section\n    query = st.text_input(\"Ask a question about the file:\")\n\n    if query:\n        # Simple query handling - check if the query exists in the text\n        if query.lower() in text.lower():\n            st.write(\"Found a match in the document!\")\n        else:\n            st.write(\"No match found. Try a different query.\")\nelse:\n    st.write(\"Please upload a PDF file to get started.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install streamlit PyPDF2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\nfrom PyPDF2 import PdfReader\n\n# Function to extract text from the uploaded PDF\ndef extract_text_from_pdf(pdf_file):\n    pdf_reader = PdfReader(pdf_file)\n    text = \"\"\n    for page_num in range(len(pdf_reader.pages)):\n        page = pdf_reader.pages[page_num]\n        text += page.extract_text()\n    return text\n\n# Set up the Streamlit app\nst.title(\"PDF File Upload and Query App\")\n\n# Upload file section\nuploaded_file = st.file_uploader(\"Choose a PDF file\", type=[\"pdf\"])\n\nif uploaded_file is not None:\n    # Extract text from the uploaded PDF\n    st.write(\"File uploaded successfully!\")\n    text = extract_text_from_pdf(uploaded_file)\n    \n    # Display part of the extracted text for reference\n    st.subheader(\"Extracted Text Preview (First 1000 characters):\")\n    st.text_area(\"Extracted Text\", text[:1000], height=300)  # Show a preview of the extracted text\n\n    # User query section\n    query = st.text_input(\"Ask a question about the file:\")\n    \n    if query:\n        # Simple query handling: check if the query exists in the text\n        if query.lower() in text.lower():\n            st.write(\"Found a match in the document!\")\n        else:\n            st.write(\"No match found. Try a different query.\")\nelse:\n    st.write(\"Please upload a PDF file to get started.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install required packages\n!pip install pyngrok --quiet\n!pip install streamlit --quiet\n\n# Step 2: Kill existing tunnels (just in case)\nfrom pyngrok import ngrok\nngrok.kill()\n\n# Step 3: Use subprocess to run Streamlit in foreground\nimport subprocess\nimport threading\n\ndef run_streamlit():\n    subprocess.run([\"streamlit\", \"run\", \"app.py\"])\n\n# Run Streamlit in a thread instead of background process\nthread = threading.Thread(target=run_streamlit)\nthread.start()\n\n# Step 4: Connect ngrok tunnel\npublic_url = ngrok.connect(8501)\nprint(f\"üåê Your app is live at: {public_url}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install google-generativeai PyPDF2 streamlit\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\n\n# Set your API key\nos.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\"\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import streamlit as st\nimport PyPDF2\nimport google.generativeai as genai\n\n# Set your Gemini API key\ngenai.configure(api_key=\"AIzaSyARQ2N2d6K7H1CDh6Cr2eyd1q0BFeiayas\")\n\nst.title(\"üìÑ PDF Question Answering App\")\n\nuploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n\nif uploaded_file:\n    st.success(\"‚úÖ PDF uploaded successfully!\")\n\n    # Extract text from PDF\n    pdf_reader = PyPDF2.PdfReader(uploaded_file)\n    raw_text = \"\"\n    for page in pdf_reader.pages:\n        raw_text += page.extract_text()\n\n    # Show question input if text was extracted\n    if raw_text.strip():\n        user_question = st.text_input(\"üí¨ Ask a question about the PDF content:\")\n\n        if user_question:\n            with st.spinner(\"Thinking... üí≠\"):\n                model = genai.GenerativeModel(\"gemini-pro\")\n                response = model.generate_content([raw_text, user_question])\n                st.markdown(\"### ü§ñ Answer:\")\n                st.write(response.text)\n    else:\n        st.warning(\"Could not extract any text from the PDF.\")\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!streamlit run app.py --server.enableXsrfProtection=false --server.port=8501\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok\nfrom pyngrok import ngrok\n\n# Start ngrok tunnel\npublic_url = ngrok.connect(8501)\nprint(\"Streamlit app URL:\", public_url)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\nprint(ngrok.connect(8501))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install streamlit pyngrok\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}